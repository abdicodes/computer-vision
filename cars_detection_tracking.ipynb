{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A Motion Detected Alarm System with Python and OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are going to develop – again! – a motion triggered alarm based on the frame differencing method. Now, we are going to do this using Python and OpenCV.\n",
    "\n",
    "Open Source Computer Vision Library (OpenCV) is a library written in C/C++ mainly aimed at real-time computer vision.\n",
    "\n",
    "OpenCV supports a wide variety of programming languages such as C++, Python, Java, etc. \n",
    "\n",
    "In particular, in this exercise we are going to introduce OpenCV-Python, a Python wrapper for the original OpenCV C++ implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach for developing this application is the same as proposed in Exercise 14:\n",
    "\n",
    "When the program starts, the program will capture from the webcam the ‘baseline’ image (see the left image below). This is the image without any 'intruder'. The program will keep capturing frames and comparing them with the baseline image. If no one enters the frame, nothing will happen. However, when somebody or something enters the frame (see the right image below), the program will detect that the captured frame and the baseline image are ‘very’ different and will trigger the audio alarm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alarm](../Images/Alarm_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very important: you need to run the code in your local machine for displaying the video and using the webcam.**\n",
    "\n",
    "**Exercise 1.1**: Download (or copy) this Jupyter notebook into your local machine.\n",
    "\n",
    "**Exercise 1.2**: Install the required Python libraries\n",
    "\n",
    "                pip install pyttsx3\n",
    "                pip install pywin32\n",
    "                pip install numpy\n",
    "                pip install opencv-python\n",
    "\n",
    "**Exercise 1.3**: Run and study the code *Capture and display the frames of a video with Python and OpenCV*.\n",
    "\n",
    "**Exercise 1.4**: Run and study the code *Capture and display webcam with Python and OpenCV*.\n",
    "\n",
    "**Exercise 1.5**: Run and study the code *Build a motion detected alarm system with Python*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3. Capture and display the frames of a video with Python and OpenCV.\n",
    "\n",
    " Run and study the code Capture and display the frames of a video with Python and OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyttsx3\n",
    "!pip install pywin32\n",
    "!pip install numpy\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture and display the frames of a video with Python and OpenCV\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Create a VideoCapture object and read the frames from an input file\n",
    "# You can download this video from https://www.loc.gov/item/00694162/ (or use another video!)\n",
    "# video=cv2.VideoCapture('media-ammem-AmericaAtWorkAmericaAtLeisure-1968.mp4')\n",
    "# video=cv2.VideoCapture('http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4')\n",
    "video = cv2.VideoCapture('./butterfly_flower_insect_nature_515.mp4')\n",
    "\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if (video.isOpened()== False): \n",
    "  print(\"Error opening video file\")\n",
    "\n",
    "# # Read until video is completed or we press 'q'\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    # Note that VideoCapture captures the frames of a video without considering the fps of the video\n",
    "    check, frame = video.read()\n",
    "\n",
    "    if check == True:\n",
    "    \n",
    "        cv2.imshow(\"movie\",frame)\n",
    "    \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "\n",
    "# After the loop release the video object\n",
    "video.release()\n",
    "\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4. Capture and display webcam with Python and OpenCV.\n",
    "\n",
    "Run and study the code Capture and display webcam with Python and OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture and display webcam with Python and OpenCV\n",
    "\n",
    "import cv2\n",
    "\n",
    "# We use VideoCapture function to create the video capture object\n",
    "# Note we put '0' to capture webcam\n",
    "video=cv2.VideoCapture(0)\n",
    "\n",
    "# We start an infinite loop and keep reading frames from the webcam until we press 'q'\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    cv2.imshow(q\"Webcam\",frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# After the loop release the video object\n",
    "video.release()\n",
    "\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5. Build a motion detected alarm system with Python.\n",
    "\n",
    "Run and study the code Build a motion detected alarm system with Python.\n",
    "\n",
    "In particular, analyse and try different parameters in the functions:\n",
    "\n",
    "- GaussianBlur(): for noise reduction (smoothening).\n",
    "- threshold(): you need to change the threshold function values for a better performance with your webcam , room's light, etc.\n",
    "- findContours(): this method identifies all the contours in an image. Experiment with the different contour retrieval modes (https://docs.opencv.org/4.5.2/d3/dc0/group__imgproc__shape.html#ga819779b9857cc2f8601e6526a3a5bc71) and contour approximation methods (https://docs.opencv.org/4.5.2/d3/dc0/group__imgproc__shape.html#ga4303f45752694956374734a03c54d5ff),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build A Motion Detected Alarm System with Python\n",
    "\n",
    "# let’s import the libraries\n",
    "# For playing the audio, we will be using “pyttsx3” python library to convert text to speech\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "# This funtion plays the audio message\n",
    "# def voice_alarm(alarm_sound):\n",
    "#     alarm_sound.say(\"Intruder Detected\")\n",
    "#     alarm_sound.runAndWait()\n",
    "    \n",
    "\n",
    "# Setting parameters for voice\n",
    "# alarm_sound = pyttsx3.init()\n",
    "# voices = alarm_sound.getProperty('voices')\n",
    "# alarm_sound.setProperty('voice', voices[0].id)\n",
    "# alarm_sound.setProperty('rate', 150)\n",
    "\n",
    "\n",
    "# The function to play the audio wil be executed in a separate thread.\n",
    "# So, there won't be lag in the video feed while the audio alert message is playing.\n",
    "# alarm = threading.Thread(target=voice_alarm, args=(alarm_sound,))\n",
    "\n",
    "\n",
    "#status_list=[None,None]\n",
    "status_list=[0,0]\n",
    "initial_frame = None\n",
    "\n",
    "\n",
    "# We use VideoCapture function to create the video capture object\n",
    "video=cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# We start an infinite loop and keep reading frames from the webcam until we press 'q'\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    status=0\n",
    "\n",
    "    # Gray conversion and noise reduction (smoothening)\n",
    "    gray_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    blur_frame=cv2.GaussianBlur(gray_frame,(25,25),0)\n",
    "\n",
    "    \n",
    "    # The first captured frame is the baseline image\n",
    "    if initial_frame is None:\n",
    "        initial_frame = blur_frame\n",
    "        continue\n",
    "\n",
    "    # The difference between the baseline and the new frame\n",
    "    delta_frame=cv2.absdiff(initial_frame,blur_frame)\n",
    "    # The difference (the delta_frame) is converted into a binary image\n",
    "    # If a particular pixel value is greater than a certain threshold (specified by us here as 150),\n",
    "    # it will be assigned the value for White (255) else Black(0)\n",
    "    # Important: you may have to change the threshold value for a better performance with your webcam , room's light, etc.\n",
    "    threshold_frame=cv2.threshold(delta_frame,180,255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "\n",
    "    # The cv2.findContours() method we will identify all the contours in our image.\n",
    "    # This method expects 3 parameters, (a) image, (b) contour retrieval mode and\n",
    "    # (c) contour approximation method\n",
    "    (contours,_)=cv2.findContours(threshold_frame,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for c in contours:\n",
    "        # contourArea() method filters out any small contours\n",
    "        # You can change this value\n",
    "        if cv2.contourArea(c) < 1000:\n",
    "            continue\n",
    "        status=status + 1\n",
    "        (x, y, w, h)=cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 1)\n",
    "    status_list.append(status)\n",
    "\n",
    "\n",
    "    # The alarm is triggered if an 'intruder' is detected\n",
    "    # We can also trigger the alarm only if a moving object is detected with\n",
    "    #if status_list[-1]>= 1 and status_list[-2]==0:    \n",
    "#     if status_list[-2]>= 1:\n",
    "#         if (alarm.is_alive() == False):\n",
    "#             alarm = threading.Thread(target=voice_alarm, args=(alarm_sound,))\n",
    "#             alarm.start()\n",
    "\n",
    "        \n",
    "    # To better understand the application, we can visualise the different frames generated\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    cv2.imshow('Baseline image', initial_frame)\n",
    "    cv2.imshow(\"Gray Frame\",gray_frame)\n",
    "    cv2.imshow('Delta frame', delta_frame)   \n",
    "    cv2.imshow('Threshold frame', threshold_frame)\n",
    "    \n",
    "\n",
    "    # Stop the program by pressing 'q'    q\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "        \n",
    "# After the loop release the video object, stop the alarm\n",
    "# and destroy all the windows\n",
    "# alarm_sound.stop()\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This lab is based on https://github.com/arindomjit/Motion_Detected_Alarm by Arindomjit Bhattacharjee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build A Motion Detected Alarm System with Python\n",
    "\n",
    "# let’s import the libraries\n",
    "# For playing the audio, we will be using “pyttsx3” python library to convert text to speech\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "# This funtion plays the audio message\n",
    "# def voice_alarm(alarm_sound):\n",
    "#     alarm_sound.say(\"Intruder Detected\")\n",
    "#     alarm_sound.runAndWait()\n",
    "    \n",
    "\n",
    "# Setting parameters for voice\n",
    "# alarm_sound = pyttsx3.init()\n",
    "# voices = alarm_sound.getProperty('voices')\n",
    "# alarm_sound.setProperty('voice', voices[0].id)\n",
    "# alarm_sound.setProperty('rate', 150)\n",
    "\n",
    "\n",
    "# The function to play the audio wil be executed in a separate thread.\n",
    "# So, there won't be lag in the video feed while the audio alert message is playing.\n",
    "# alarm = threading.Thread(target=voice_alarm, args=(alarm_sound,))\n",
    "\n",
    "\n",
    "#status_list=[None,None]\n",
    "status_list=[0,0]\n",
    "initial_frame = None\n",
    "\n",
    "\n",
    "# We use VideoCapture function to create the video capture object\n",
    "video=cv2.VideoCapture('Exercise1_Files/Traffic_Laramie_1.mp4')\n",
    "\n",
    "\n",
    "# We start an infinite loop and keep reading frames from the webcam until we press 'q'\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    status=0\n",
    "    height, width , _ = frame.shape\n",
    "\n",
    "\n",
    "    main_street = frame[int(height/2): height, 0:width]\n",
    "\n",
    "\n",
    "\n",
    "    # Gray conversion and noise reduction (smoothening)\n",
    "    gray_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    blur_frame=cv2.GaussianBlur(gray_frame,(25,25),0)\n",
    "\n",
    "    \n",
    "    # The first captured frame is the baseline image\n",
    "    if initial_frame is None:\n",
    "        initial_frame = blur_frame\n",
    "        continue\n",
    "\n",
    "    # The difference between the baseline and the new frame\n",
    "    delta_frame=cv2.absdiff(initial_frame,blur_frame)\n",
    "    # The difference (the delta_frame) is converted into a binary image\n",
    "    # If a particular pixel value is greater than a certain threshold (specified by us here as 150),\n",
    "    # it will be assigned the value for White (255) else Black(0)\n",
    "    # Important: you may have to change the threshold value for a better performance with your webcam , room's light, etc.\n",
    "    threshold_frame=cv2.threshold(delta_frame,50,255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "\n",
    "    # The cv2.findContours() method we will identify all the contours in our image.\n",
    "    # This method expects 3 parameters, (a) image, (b) contour retrieval mode and\n",
    "    # (c) contour approximation method\n",
    "    (contours,_)=cv2.findContours(threshold_frame,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for c in contours:\n",
    "        # contourArea() method filters out any small contours\n",
    "        # You can change this value\n",
    "        if cv2.contourArea(c) < 1000:\n",
    "            continue\n",
    "        status=status + 1\n",
    "        (x, y, w, h)=cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 1)\n",
    "    status_list.append(status)\n",
    "\n",
    "\n",
    "    # The alarm is triggered if an 'intruder' is detected\n",
    "    # We can also trigger the alarm only if a moving object is detected with\n",
    "    #if status_list[-1]>= 1 and status_list[-2]==0:    \n",
    "#     if status_list[-2]>= 1:\n",
    "#         if (alarm.is_alive() == False):\n",
    "#             alarm = threading.Thread(target=voice_alarm, args=(alarm_sound,))\n",
    "#             alarm.start()\n",
    "\n",
    "        \n",
    "    # To better understand the application, we can visualise the different frames generated\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    cv2.imshow('Baseline image', initial_frame)\n",
    "    cv2.imshow(\"Gray Frame\",gray_frame)\n",
    "    cv2.imshow('Delta frame', delta_frame)   \n",
    "    cv2.imshow('Threshold frame', threshold_frame)\n",
    "    cv2.imshow('Main Street', main_street)\n",
    "    \n",
    "\n",
    "    # Stop the program by pressing 'q'    q\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "        \n",
    "# After the loop release the video object, stop the alarm\n",
    "# and destroy all the windows\n",
    "# alarm_sound.stop()\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap=cv2.VideoCapture('Exercise1_Files/Traffic_Laramie_1.mp4')\n",
    "\n",
    "\n",
    "# Capture the first frame and use it as the background image\n",
    "_, bg = cap.read()\n",
    "bg = cv2.cvtColor(bg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the current frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     print(height, width)\n",
    "\n",
    "    # Perform background subtraction to obtain the foreground mask\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "    fgmask = fgbg.apply(gray)\n",
    "\n",
    "    # Perform frame differencing to obtain the difference mask\n",
    "    diff = cv2.absdiff(gray, bg)\n",
    "\n",
    "    # Combine the foreground mask and the difference mask to get the final mask\n",
    "    mask = cv2.bitwise_and(fgmask, diff)\n",
    "\n",
    "    # Apply the final mask on the current frame to obtain the foreground objects\n",
    "    res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "\n",
    "    # Update the background image for the next frame\n",
    "    bg = gray\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('../../Downloads//Exercise1_Files/Traffic_Laramie_1.mp4')\n",
    "\n",
    "# Create background subtractor object\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Capture the first frame and use it as the background image\n",
    "_, bg = cap.read()\n",
    "height, width , _ = bg.shape\n",
    "bg = cv2.cvtColor(bg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "bg = bg[int(height/2): height, 0:width]\n",
    "\n",
    "\n",
    "# Initialize list to hold trackers for each car\n",
    "trackers = []\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    height, width , _ = frame.shape\n",
    "    main_street = frame[int(height/2): height, 0:width]\n",
    "    \n",
    "    # Convert the current frame to grayscale\n",
    "    gray = cv2.cvtColor(main_street, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    # Perform background subtraction to obtain the foreground mask\n",
    "    fgmask = fgbg.apply(gray)\n",
    "\n",
    "    # Perform frame differencing to obtain the difference mask\n",
    "    diff = cv2.absdiff(gray, bg)\n",
    "\n",
    "    # Combine the foreground mask and the difference mask to get the final mask\n",
    "    mask = cv2.bitwise_and(fgmask, diff)\n",
    "\n",
    "    # Apply morphological operations to improve mask quality\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "#     mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours in the final mask\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Loop over the contours to filter the cars and create bounding boxes\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) < 2000:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        aspect_ratio = w / h\n",
    "        if area > 7000 and aspect_ratio > 0.3 and aspect_ratio < 3:\n",
    "\n",
    "\n",
    "#         (x, y, w, h)=cv2.boundingRect(c)\n",
    "            cv2.rectangle(frame, (x, y + int(height/2)), (x+w, y+h + int(height/2)), (0,255,0), 1)\n",
    "\n",
    "#         # Filter based on size, aspect ratio, and position\n",
    "#         if area > 500 and aspect_ratio > 0.3 and aspect_ratio < 3 and y > 100 and y < 400:\n",
    "#             # Initialize a new tracker for the car\n",
    "#             tracker = cv2.TrackerCSRT_create()\n",
    "#             tracker.init(frame, (x, y, w, h))\n",
    "#             trackers.append(tracker)\n",
    "\n",
    "    # Update the tracker for each car and draw the bounding box\n",
    "#     for tracker in trackers:\n",
    "#         ok, bbox = tracker.update(frame)\n",
    "#         if ok:\n",
    "#             x, y, w, h = [int(v) for v in bbox]\n",
    "#             cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Mask', mask)\n",
    "    cv2.imshow('Main street', main_street)\n",
    "\n",
    "    # Update the background image for the next frame\n",
    "    bg = gray\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traffic counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('../../Downloads//Exercise1_Files/Traffic_Laramie_2.mp4')\n",
    "\n",
    "# Create background subtractor object\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Capture the first frame and use it as the background image\n",
    "_, bg = cap.read()\n",
    "height, width , _ = bg.shape\n",
    "\n",
    "x_offset = 150\n",
    "y_offset = 170\n",
    "x_start_point = int(width/2) - x_offset\n",
    "x_end_point = int(width/2 )\n",
    "y_start_point = int(height/2)\n",
    "y_end_point = int(height/2) + y_offset\n",
    "bg = cv2.cvtColor(bg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "bg = bg[y_start_point: y_end_point,x_start_point:x_end_point]\n",
    "\n",
    "\n",
    "    trackers = []\n",
    "\n",
    "# Initialize multi-object tracker\n",
    "tracker = cv2.legacy.MultiTracker_create()\n",
    "\n",
    "# Initialize variables to keep track of cars\n",
    "car_count = 0\n",
    "cars_passed = set()\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    height, width , _ = frame.shape\n",
    "    city_center_street = frame[int(height/2): (int(height/2) + y_offset), int(width/2) - x_offset:int(width/2 )]\n",
    "    \n",
    "    # Convert the current frame to grayscale\n",
    "    gray = cv2.cvtColor(city_center_street, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    # Perform background subtraction to obtain the foreground mask\n",
    "    fgmask = fgbg.apply(gray)\n",
    "\n",
    "    # Perform frame differencing to obtain the difference mask\n",
    "    diff = cv2.absdiff(gray, bg)\n",
    "\n",
    "    # Combine the foreground mask and the difference mask to get the final mask\n",
    "    mask = cv2.bitwise_and(fgmask, diff)\n",
    "\n",
    "    # Apply morphological operations to improve mask quality\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "#     mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours in the final mask\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "  \n",
    "\n",
    "    # Loop over the contours to filter the cars and create bounding boxes\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) < 2000:\n",
    "            continue\n",
    "            \n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        aspect_ratio = w / h\n",
    "        if area > 7000 and aspect_ratio > 0.3 and aspect_ratio < 3:\n",
    "\n",
    "\n",
    "#         (x, y, w, h)=cv2.boundingRect(c)\n",
    "            x=  x + x_start_point\n",
    "            y = y + y_start_point\n",
    "           \n",
    "            \n",
    "\n",
    "            cv2.rectangle(frame, (x, y ), (x+w, y+h), (0,255,0), 1)\n",
    "            \n",
    "            bbox = (x , y , w, h)\n",
    "            tracker.add(cv2.legacy.TrackerCSRT_create(), frame, bbox)\n",
    "    print(tracker.update(frame))\n",
    "    for i, newbox in enumerate(tracker.update(frame)):\n",
    "        x, y, w, h = [int(v) for v in newbox]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Check if the object has passed the line and increment the car count\n",
    "        if x + w // 2 > x2 and i not in cars_passed:\n",
    "            car_count += 1\n",
    "            cars_passed.add(i)\n",
    "\n",
    "#         # Filter based on size, aspect ratio, and position\n",
    "#         if area > 500 and aspect_ratio > 0.3 and aspect_ratio < 3 and y > 100 and y < 400:\n",
    "#             # Initialize a new tracker for the car\n",
    "#             tracker = cv2.TrackerCSRT_create()\n",
    "#             tracker.init(frame, (x, y, w, h))\n",
    "#             trackers.append(tracker)\n",
    "\n",
    "    # Update the tracker for each car and draw the bounding box\n",
    "#     for tracker in trackers:\n",
    "#         ok, bbox = tracker.update(frame)\n",
    "#         if ok:\n",
    "#             x, y, w, h = [int(v) for v in bbox]\n",
    "#             cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.rectangle(frame, (int((x_start_point + x_end_point)/2), y_start_point  ), (int((x_start_point + x_end_point)/2), y_end_point), (0,255,0), 1)\n",
    "    # Show the result\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Mask', mask)\n",
    "    cv2.imshow(' Counting point', city_center_street)\n",
    "    cv2.putText(frame, f\"Cars passed: {car_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Update the background image for the next frame\n",
    "    bg = gray\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindos()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "tracker_types = ['BOOSTING', 'MIL','KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "tracker_type = tracker_types[5]\n",
    "\n",
    "if tracker_type == 'BOOSTING':\n",
    "    tracker = cv2.legacy.TrackerBoosting_create()\n",
    "if tracker_type == 'MIL':\n",
    "    tracker = cv2.TrackerMIL_create() \n",
    "if tracker_type == 'KCF':\n",
    "    tracker = cv2.TrackerKCF_create() \n",
    "if tracker_type == 'TLD':\n",
    "    tracker = cv2.legacy.TrackerTLD_create() \n",
    "if tracker_type == 'MEDIANFLOW':\n",
    "    tracker = cv2.legacy.TrackerMedianFlow_create() \n",
    "if tracker_type == 'MOSSE':\n",
    "    tracker = cv2.legacy.TrackerMOSSE_create()\n",
    "if tracker_type == \"CSRT\":\n",
    "    tracker = cv2.TrackerCSRT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.legacy.TrackerBoosting_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.TrackerMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('../../Downloads//Exercise1_Files/Traffic_Laramie_2.mp4')\n",
    "\n",
    "# Create background subtractor object\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Capture the first frame and use it as the background image\n",
    "_, bg = cap.read()\n",
    "bg = cv2.cvtColor(bg, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initialize list to hold trackers for each car\n",
    "trackers = []\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the current frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform background subtraction to obtain the foreground mask\n",
    "    fgmask = fgbg.apply(gray)\n",
    "\n",
    "    # Perform frame differencing to obtain the difference mask\n",
    "    diff = cv2.absdiff(gray, bg)\n",
    "\n",
    "    # Combine the foreground mask and the difference mask to get the final mask\n",
    "    mask = cv2.bitwise_and(fgmask, diff)\n",
    "\n",
    "    # Apply morphological operations to improve mask quality\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours in the final mask\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Loop over the contours to filter the cars and create bounding boxes\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        aspect_ratio = w / h\n",
    "\n",
    "        # Filter based on size, aspect ratio, and position\n",
    "        if area > 6000 and aspect_ratio > 0.3 and aspect_ratio < 3 and y > 100 and y < 400:\n",
    "            # Initialize a new tracker for the car\n",
    "            tracker = cv2.legacy.TrackerCSRT_create()\n",
    "            tracker.init(frame, (x, y, w, h))\n",
    "            trackers.append(tracker)\n",
    "\n",
    "    # Update the tracker for each car and draw the bounding box\n",
    "    for tracker in trackers:\n",
    "        ok, bbox = tracker.update(frame)\n",
    "        if ok:\n",
    "            x, y, w, h = [int(v) for v in bbox]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Update the background image for the next frame\n",
    "    bg = gray\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\n",
      "Found existing installation: opencv-contrib-python 4.5.5.62\n",
      "Uninstalling opencv-contrib-python-4.5.5.62:\n",
      "  Would remove:\n",
      "    /Users/user/opt/anaconda3/lib/python3.8/site-packages/cv2/*\n",
      "    /Users/user/opt/anaconda3/lib/python3.8/site-packages/opencv_contrib_python-4.5.5.62.dist-info/*\n",
      "  Would not remove (might be manually added):\n",
      "    /Users/user/opt/anaconda3/lib/python3.8/site-packages/cv2/.DS_Store\n",
      "Proceed (y/n)? "
     ]
    }
   ],
   "source": [
    "!pip uninstall opencv-python\n",
    "!pip uninstall opencv-contrib-python\n",
    "!pip uninstall opencv-contrib-python-headle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('../../Downloads//Exercise1_Files/Traffic_Laramie_2.mp4')\n",
    "\n",
    "# Create background subtractor object\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Capture the first frame and use it as the background image\n",
    "_, bg = cap.read()\n",
    "height, width , _ = bg.shape\n",
    "\n",
    "x_offset = 150\n",
    "y_offset = 170\n",
    "x_start_point = int(width/2) - x_offset\n",
    "x_end_point = int(width/2 )\n",
    "y_start_point = int(height/2)\n",
    "y_end_point = int(height/2) + y_offset\n",
    "bg = cv2.cvtColor(bg, cv2.COLOR_BGR2GRAY)\n",
    "tracker = cv2.legacy.TrackerCSRT_create()\n",
    "tracker.init(bg, (x_start_point, y_start_point, x_offset, y_offset))\n",
    "\n",
    "\n",
    "bg = bg[y_start_point: y_end_point,x_start_point:x_end_point]\n",
    "\n",
    "# Initialize list to hold trackers for each car\n",
    "trackers = cv2.legacy.MultiTracker_create()\n",
    "my_tracker = cv2.legacy.TrackerCSRT_create()\n",
    "\n",
    "\n",
    "prev_loc = []\n",
    "curr_loc = []\n",
    "count = 0 \n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    height, width , _ = frame.shape\n",
    "    city_center_street = frame[int(height/2): (int(height/2) + y_offset), int(width/2) - x_offset:int(width/2 )]\n",
    "    \n",
    "    # Convert the current frame to grayscale\n",
    "    gray = cv2.cvtColor(city_center_street, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform background subtraction to obtain the foreground mask\n",
    "    fgmask = fgbg.apply(gray)\n",
    "\n",
    "    # Perform frame differencing to obtain the difference mask\n",
    "    diff = cv2.absdiff(gray, bg)\n",
    "\n",
    "    # Combine the foreground mask and the difference mask to get the final mask\n",
    "    mask = cv2.bitwise_and(fgmask, diff)\n",
    "\n",
    "    # Find contours in the final mask\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    curr_loc = []\n",
    "    x = y = w = h= 0\n",
    "   \n",
    "\n",
    "    # Loop over the contours to filter the cars and create bounding boxes\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) < 2000:\n",
    "            continue\n",
    "            \n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "       \n",
    "        area = w * h\n",
    "        aspect_ratio = w / h\n",
    "        if area > 6500 and aspect_ratio > 0.3 and aspect_ratio < 3:\n",
    "\n",
    "            x=  x + x_start_point\n",
    "            y = y + y_start_point\n",
    "           \n",
    "            cv2.rectangle(frame, (x, y ), (x+w, y+h), (0,255,0), 1)\n",
    "            curr_loc.extend((x,y))\n",
    "            \n",
    "    if len(prev_loc) > 0 and len(curr_loc) == 0:\n",
    "        print('car passed')\n",
    "        count+=1\n",
    "\n",
    "    ok, bbox = tracker.update(frame)\n",
    "\n",
    "           \n",
    "    cv2.rectangle(frame, (int((x_start_point + x_end_point)/2), y_start_point  ), (int((x_start_point + x_end_point)/2), y_end_point), (0,255,0), 1)\n",
    "    # Show the result\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow(' Counting point', city_center_street)\n",
    "    cv2.putText(frame, f\"Cars passed: {count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Update the background image for the next frame\n",
    "    bg = gray\n",
    "    prev_loc = curr_loc.copy()\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindos()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import math Library\n",
    "import math \n",
    "\n",
    "p = [3] \n",
    "q = [1] \n",
    "\n",
    "# Calculate Euclidean distance\n",
    "print (math.dist(p, q))\n",
    "\n",
    "p = [5,1] \n",
    "q = [0, 12] \n",
    "\n",
    "# Calculate Euclidean distance\n",
    "print (math.dist(p, q))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test.extend((3,4))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
